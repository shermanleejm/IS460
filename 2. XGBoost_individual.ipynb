{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer,\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    mean_squared_error,\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensim_api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words(\"english\"))\n",
    "spacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "train = pd.read_csv(\"train_evenly_distributed.csv\")\n",
    "evaluation = pd.read_csv(\"test.csv\")\n",
    "evaluation_labels = pd.read_csv(\"test_labels.csv\")\n",
    "\n",
    "model = XGBClassifier(random_state=69, seed=2, colsample_bytree=0.6, subsample=0.7)\n",
    "\n",
    "param_grid = {\n",
    "    \"clf__n_estimators\": [50, 100, 300],\n",
    "    \"clf__colsample_bytree\": [0.6, 0.8, 1],\n",
    "    \"clf__subsample\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCleaned and ran it once, stored in \"clean.csv\" \\nTakes about 15 min on my com\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cleaned and ran it once, stored in \"clean.csv\" \n",
    "Takes about 15 min on my com\n",
    "\"\"\"\n",
    "\n",
    "## clean, stem and generate word set\n",
    "# def clean(text):\n",
    "#     text = text.lower()\n",
    "#     ## remove \\n \\t and non-alphanumeric\n",
    "#     text = re.sub(\"(\\\\t|\\\\n)\", \" \", text)\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     ## remove empty tokens\n",
    "#     text = \" \".join([x.strip() for x in text.split(\" \") if len(x.strip()) > 0])\n",
    "#     ## lemmatise\n",
    "#     doc = spacy_nlp(text)\n",
    "#     text = \" \".join([x.lemma_ for x in doc if not x.is_stop])\n",
    "#     return text.strip()\n",
    "\n",
    "\n",
    "# train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: clean(x))\n",
    "\n",
    "# with open(\"clean.csv\", \"w+\") as f:\n",
    "#     train.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a new model for each category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pd.read_csv(\"train_evenly_distributed.csv\")\n",
    "clean.dropna(inplace=True)\n",
    "clean[\"comment_text\"] = clean[\"comment_text\"].str.replace(\",\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boring TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8429696287964005\n",
      "0.8429696287964005\n",
      "0.8429696287964005\n",
      "0.8429696287964005\n",
      "0.8429696287964005\n",
      "0.8429696287964005\n"
     ]
    }
   ],
   "source": [
    "## without glove and basic tfidf\n",
    "\n",
    "## old pipeline\n",
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"vect\", CountVectorizer()),\n",
    "#         (\"tfidf\", TfidfTransformer()),\n",
    "#         (\"classifier\", LogisticRegression()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "## new pipeline but super slow\n",
    "pipe = Pipeline([(\"classifier\", LogisticRegression())])\n",
    "\n",
    "vectoriser = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_features=100000,\n",
    "    ngram_range=(1, 2),\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "vectors = vectoriser.fit_transform(clean[\"comment_text\"])\n",
    "manual_train = pd.DataFrame(\n",
    "    data=vectors.toarray(), columns=vectoriser.get_feature_names()\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    manual_train, clean[tox], test_size=0.20, random_state=69\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"classifier\": [LogisticRegression()],\n",
    "        \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "        \"classifier__C\": np.logspace(-4, 4, 20),\n",
    "        \"classifier__solver\": [\"liblinear\"],\n",
    "        \"classifier__max_iter\": [1000, 5000],\n",
    "    },\n",
    "]\n",
    "\n",
    "log_reg_models = {}\n",
    "for tox in toxic_labels:\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     clean[\"comment_text\"], clean[tox], test_size=0.20, random_state=69\n",
    "    # )\n",
    "    # model = GridSearchCV(pipe, param_grid=param_grid, cv=3, verbose=False, n_jobs=1)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.score(X_test, y_test))\n",
    "    with open(f\"./models/{tox}.sav\", \"wb+\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    # log_reg_models[tox] = model.best_params_[\"classifier\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': LogisticRegression(C=1.623776739188721, max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear'),\n",
       " 'severe_toxic': LogisticRegression(C=1.623776739188721, max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear'),\n",
       " 'obscene': LogisticRegression(C=1.623776739188721, max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear'),\n",
       " 'threat': LogisticRegression(C=1.623776739188721, max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear'),\n",
       " 'insult': LogisticRegression(C=1.623776739188721, max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear'),\n",
       " 'identity_hate': LogisticRegression(C=1.623776739188721, max_iter=1000, penalty='l1',\n",
       "                    solver='liblinear')}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work'],\n",
       " ['hey',\n",
       "  'what',\n",
       "  'is',\n",
       "  'it',\n",
       "  'talk',\n",
       "  'what',\n",
       "  'is',\n",
       "  'it',\n",
       "  'an',\n",
       "  'exclusive',\n",
       "  'group',\n",
       "  'of',\n",
       "  'some',\n",
       "  'wp',\n",
       "  'talibans',\n",
       "  'who',\n",
       "  'are',\n",
       "  'good',\n",
       "  'at',\n",
       "  'destroying',\n",
       "  'self',\n",
       "  'appointed',\n",
       "  'purist',\n",
       "  'who',\n",
       "  'gang',\n",
       "  'up',\n",
       "  'any',\n",
       "  'one',\n",
       "  'who',\n",
       "  'asks',\n",
       "  'them',\n",
       "  'questions',\n",
       "  'abt',\n",
       "  'their',\n",
       "  'anti',\n",
       "  'social',\n",
       "  'and',\n",
       "  'destructive',\n",
       "  'non',\n",
       "  'contribution',\n",
       "  'at',\n",
       "  'wp',\n",
       "  'ask',\n",
       "  'sityush',\n",
       "  'to',\n",
       "  'clean',\n",
       "  'up',\n",
       "  'his',\n",
       "  'behavior',\n",
       "  'than',\n",
       "  'issue',\n",
       "  'me',\n",
       "  'nonsensical',\n",
       "  'warnings'],\n",
       " ['bye',\n",
       "  'don',\n",
       "  't',\n",
       "  'look',\n",
       "  'come',\n",
       "  'or',\n",
       "  'think',\n",
       "  'of',\n",
       "  'comming',\n",
       "  'back',\n",
       "  'tosser'],\n",
       " ['you',\n",
       "  'are',\n",
       "  'gay',\n",
       "  'or',\n",
       "  'antisemmitian',\n",
       "  'archangel',\n",
       "  'white',\n",
       "  'tiger',\n",
       "  'meow',\n",
       "  'greetingshhh',\n",
       "  'uh',\n",
       "  'there',\n",
       "  'are',\n",
       "  'two',\n",
       "  'ways',\n",
       "  'why',\n",
       "  'you',\n",
       "  'do',\n",
       "  'erased',\n",
       "  'my',\n",
       "  'comment',\n",
       "  'about',\n",
       "  'ww',\n",
       "  'that',\n",
       "  'holocaust',\n",
       "  'was',\n",
       "  'brutally',\n",
       "  'slaying',\n",
       "  'of',\n",
       "  'jews',\n",
       "  'and',\n",
       "  'not',\n",
       "  'gays',\n",
       "  'gypsys',\n",
       "  'slavs',\n",
       "  'anyone',\n",
       "  'if',\n",
       "  'you',\n",
       "  'are',\n",
       "  'anti',\n",
       "  'semitian',\n",
       "  'than',\n",
       "  'shave',\n",
       "  'your',\n",
       "  'head',\n",
       "  'bald',\n",
       "  'and',\n",
       "  'go',\n",
       "  'to',\n",
       "  'the',\n",
       "  'skinhead',\n",
       "  'meetings',\n",
       "  'if',\n",
       "  'you',\n",
       "  'doubt',\n",
       "  'words',\n",
       "  'of',\n",
       "  'the',\n",
       "  'bible',\n",
       "  'that',\n",
       "  'homosexuality',\n",
       "  'is',\n",
       "  'a',\n",
       "  'deadly',\n",
       "  'sin',\n",
       "  'make',\n",
       "  'a',\n",
       "  'pentagram',\n",
       "  'tatoo',\n",
       "  'on',\n",
       "  'your',\n",
       "  'forehead',\n",
       "  'go',\n",
       "  'to',\n",
       "  'the',\n",
       "  'satanistic',\n",
       "  'masses',\n",
       "  'with',\n",
       "  'your',\n",
       "  'gay',\n",
       "  'pals',\n",
       "  'first',\n",
       "  'and',\n",
       "  'last',\n",
       "  'warning',\n",
       "  'you',\n",
       "  'fucking',\n",
       "  'gay',\n",
       "  'i',\n",
       "  'won',\n",
       "  't',\n",
       "  'appreciate',\n",
       "  'if',\n",
       "  'any',\n",
       "  'more',\n",
       "  'nazi',\n",
       "  'shwain',\n",
       "  'would',\n",
       "  'write',\n",
       "  'in',\n",
       "  'my',\n",
       "  'page',\n",
       "  'i',\n",
       "  'don',\n",
       "  't',\n",
       "  'wish',\n",
       "  'to',\n",
       "  'talk',\n",
       "  'to',\n",
       "  'you',\n",
       "  'anymore',\n",
       "  'beware',\n",
       "  'of',\n",
       "  'the',\n",
       "  'dark',\n",
       "  'side'],\n",
       " ['fuck', 'your', 'filthy', 'mother', 'in', 'the', 'ass', 'dry'],\n",
       " ['i',\n",
       "  'm',\n",
       "  'sorry',\n",
       "  'i',\n",
       "  'm',\n",
       "  'sorry',\n",
       "  'i',\n",
       "  'screwed',\n",
       "  'around',\n",
       "  'with',\n",
       "  'someones',\n",
       "  'talk',\n",
       "  'page',\n",
       "  'it',\n",
       "  'was',\n",
       "  'very',\n",
       "  'bad',\n",
       "  'to',\n",
       "  'do',\n",
       "  'i',\n",
       "  'know',\n",
       "  'how',\n",
       "  'having',\n",
       "  'the',\n",
       "  'templates',\n",
       "  'on',\n",
       "  'their',\n",
       "  'talk',\n",
       "  'page',\n",
       "  'helps',\n",
       "  'you',\n",
       "  'assert',\n",
       "  'your',\n",
       "  'dominance',\n",
       "  'over',\n",
       "  'them',\n",
       "  'i',\n",
       "  'know',\n",
       "  'i',\n",
       "  'should',\n",
       "  'bow',\n",
       "  'down',\n",
       "  'to',\n",
       "  'the',\n",
       "  'almighty',\n",
       "  'administrators',\n",
       "  'but',\n",
       "  'then',\n",
       "  'again',\n",
       "  'i',\n",
       "  'm',\n",
       "  'going',\n",
       "  'to',\n",
       "  'go',\n",
       "  'play',\n",
       "  'outside',\n",
       "  'with',\n",
       "  'your',\n",
       "  'mom'],\n",
       " ['get',\n",
       "  'fucked',\n",
       "  'up',\n",
       "  'get',\n",
       "  'fuckeeed',\n",
       "  'up',\n",
       "  'got',\n",
       "  'a',\n",
       "  'drink',\n",
       "  'that',\n",
       "  'you',\n",
       "  'cant',\n",
       "  'put',\n",
       "  'down',\n",
       "  'get',\n",
       "  'fuck',\n",
       "  'up',\n",
       "  'get',\n",
       "  'fucked',\n",
       "  'up',\n",
       "  'i',\n",
       "  'm',\n",
       "  'fucked',\n",
       "  'up',\n",
       "  'right',\n",
       "  'now'],\n",
       " ['stupid',\n",
       "  'peace',\n",
       "  'of',\n",
       "  'shit',\n",
       "  'stop',\n",
       "  'deleting',\n",
       "  'my',\n",
       "  'stuff',\n",
       "  'asshole',\n",
       "  'go',\n",
       "  'die',\n",
       "  'and',\n",
       "  'fall',\n",
       "  'in',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'go',\n",
       "  'to',\n",
       "  'hell'],\n",
       " ['tony',\n",
       "  'sidaway',\n",
       "  'is',\n",
       "  'obviously',\n",
       "  'a',\n",
       "  'fistfuckee',\n",
       "  'he',\n",
       "  'loves',\n",
       "  'an',\n",
       "  'arm',\n",
       "  'up',\n",
       "  'his',\n",
       "  'ass'],\n",
       " ['my',\n",
       "  'band',\n",
       "  'page',\n",
       "  's',\n",
       "  'deletion',\n",
       "  'you',\n",
       "  'thought',\n",
       "  'i',\n",
       "  'was',\n",
       "  'gone',\n",
       "  'deleting',\n",
       "  'the',\n",
       "  'comment',\n",
       "  'i',\n",
       "  'posted',\n",
       "  'on',\n",
       "  'your',\n",
       "  'talk',\n",
       "  'page',\n",
       "  'does',\n",
       "  'not',\n",
       "  'delete',\n",
       "  'my',\n",
       "  'feelings',\n",
       "  'for',\n",
       "  'your',\n",
       "  'hasty',\n",
       "  'decision',\n",
       "  'to',\n",
       "  'delete',\n",
       "  'my',\n",
       "  'page',\n",
       "  'you',\n",
       "  'sir',\n",
       "  'are',\n",
       "  'still',\n",
       "  'a',\n",
       "  'cock',\n",
       "  'sucking',\n",
       "  'douche',\n",
       "  'fuck',\n",
       "  'sit',\n",
       "  'back',\n",
       "  'for',\n",
       "  'a',\n",
       "  'second',\n",
       "  'and',\n",
       "  'think',\n",
       "  'about',\n",
       "  'what',\n",
       "  'your',\n",
       "  'life',\n",
       "  'has',\n",
       "  'become',\n",
       "  'done',\n",
       "  'that',\n",
       "  'didn',\n",
       "  't',\n",
       "  'take',\n",
       "  'long',\n",
       "  'did',\n",
       "  'it',\n",
       "  'nope',\n",
       "  'because',\n",
       "  'as',\n",
       "  'i',\n",
       "  'am',\n",
       "  'most',\n",
       "  'certainly',\n",
       "  'aware',\n",
       "  'your',\n",
       "  'life',\n",
       "  'is',\n",
       "  'a',\n",
       "  'shitstorm',\n",
       "  'of',\n",
       "  'sitting',\n",
       "  'in',\n",
       "  'front',\n",
       "  'of',\n",
       "  'the',\n",
       "  'computer',\n",
       "  'masturbating',\n",
       "  'to',\n",
       "  'fictional',\n",
       "  'creatures',\n",
       "  'your',\n",
       "  'attempts',\n",
       "  'to',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'of',\n",
       "  'me',\n",
       "  'are',\n",
       "  'mediocre',\n",
       "  'at',\n",
       "  'best',\n",
       "  'you',\n",
       "  'are',\n",
       "  'constantly',\n",
       "  'sucking',\n",
       "  'the',\n",
       "  'dick',\n",
       "  'of',\n",
       "  'failure',\n",
       "  'you',\n",
       "  'don',\n",
       "  't',\n",
       "  'want',\n",
       "  'a',\n",
       "  'personal',\n",
       "  'attack',\n",
       "  'huh',\n",
       "  'well',\n",
       "  'too',\n",
       "  'bad',\n",
       "  'fuckcock',\n",
       "  'mcdickerson',\n",
       "  'you',\n",
       "  've',\n",
       "  'got',\n",
       "  'one',\n",
       "  'from',\n",
       "  'me',\n",
       "  'you',\n",
       "  'can',\n",
       "  'go',\n",
       "  'ahead',\n",
       "  'and',\n",
       "  'delete',\n",
       "  'my',\n",
       "  'profile',\n",
       "  'from',\n",
       "  'wikipedia',\n",
       "  'i',\n",
       "  'll',\n",
       "  'just',\n",
       "  'make',\n",
       "  'another',\n",
       "  'one',\n",
       "  'and',\n",
       "  'come',\n",
       "  'right',\n",
       "  'back',\n",
       "  'to',\n",
       "  'syrthiss',\n",
       "  's',\n",
       "  'talk',\n",
       "  'page',\n",
       "  'and',\n",
       "  'insult',\n",
       "  'the',\n",
       "  'dick',\n",
       "  'off',\n",
       "  'of',\n",
       "  'you',\n",
       "  'how',\n",
       "  'could',\n",
       "  'you',\n",
       "  'shatter',\n",
       "  'the',\n",
       "  'dreams',\n",
       "  'of',\n",
       "  'an',\n",
       "  'innocent',\n",
       "  'eighteen',\n",
       "  'year',\n",
       "  'old',\n",
       "  'college',\n",
       "  'freshman',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'make',\n",
       "  'a',\n",
       "  'name',\n",
       "  'for',\n",
       "  'his',\n",
       "  'band',\n",
       "  'does',\n",
       "  'that',\n",
       "  'make',\n",
       "  'you',\n",
       "  'happy',\n",
       "  'fucking',\n",
       "  'with',\n",
       "  'people',\n",
       "  'because',\n",
       "  'you',\n",
       "  're',\n",
       "  'an',\n",
       "  'overweight',\n",
       "  'single',\n",
       "  'old',\n",
       "  'man',\n",
       "  'in',\n",
       "  'a',\n",
       "  'dead',\n",
       "  'end',\n",
       "  'job',\n",
       "  'did',\n",
       "  'you',\n",
       "  'spot',\n",
       "  'that',\n",
       "  'perhaps',\n",
       "  'someone',\n",
       "  'else',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'follow',\n",
       "  'his',\n",
       "  'dreams',\n",
       "  'and',\n",
       "  'you',\n",
       "  'were',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'hold',\n",
       "  'him',\n",
       "  'back',\n",
       "  'so',\n",
       "  'somebody',\n",
       "  'else',\n",
       "  'could',\n",
       "  'suffer',\n",
       "  'like',\n",
       "  'you',\n",
       "  'yes',\n",
       "  'you',\n",
       "  'did',\n",
       "  'i',\n",
       "  'don',\n",
       "  't',\n",
       "  'make',\n",
       "  'empty',\n",
       "  'threats',\n",
       "  'so',\n",
       "  'i',\n",
       "  'won',\n",
       "  't',\n",
       "  'be',\n",
       "  'saying',\n",
       "  'anything',\n",
       "  'along',\n",
       "  'the',\n",
       "  'lines',\n",
       "  'of',\n",
       "  'i',\n",
       "  'll',\n",
       "  'hurt',\n",
       "  'you',\n",
       "  'or',\n",
       "  'i',\n",
       "  'll',\n",
       "  'eat',\n",
       "  'the',\n",
       "  'children',\n",
       "  'from',\n",
       "  'within',\n",
       "  'your',\n",
       "  'sister',\n",
       "  's',\n",
       "  'womb',\n",
       "  'but',\n",
       "  'i',\n",
       "  'will',\n",
       "  'say',\n",
       "  'that',\n",
       "  'you',\n",
       "  'are',\n",
       "  'a',\n",
       "  'asshole',\n",
       "  'son',\n",
       "  'of',\n",
       "  'a',\n",
       "  'bitch',\n",
       "  'mother',\n",
       "  'fucking',\n",
       "  'cock',\n",
       "  'sucker',\n",
       "  'so',\n",
       "  'go',\n",
       "  'eat',\n",
       "  'some',\n",
       "  'more',\n",
       "  'food',\n",
       "  'and',\n",
       "  'drown',\n",
       "  'your',\n",
       "  'sorrows',\n",
       "  'you',\n",
       "  'premature',\n",
       "  'ejaculating',\n",
       "  'bald',\n",
       "  'headed',\n",
       "  'fuck',\n",
       "  'you',\n",
       "  'should',\n",
       "  'do',\n",
       "  'something',\n",
       "  'nice',\n",
       "  'for',\n",
       "  'yourself',\n",
       "  'maybe',\n",
       "  'go',\n",
       "  'grab',\n",
       "  'a',\n",
       "  'couple',\n",
       "  'of',\n",
       "  'horny',\n",
       "  'goat',\n",
       "  'weeds',\n",
       "  'from',\n",
       "  'your',\n",
       "  'local',\n",
       "  'convenience',\n",
       "  'store',\n",
       "  'and',\n",
       "  'jack',\n",
       "  'off',\n",
       "  'for',\n",
       "  'a',\n",
       "  'little',\n",
       "  'longer',\n",
       "  'than',\n",
       "  'three',\n",
       "  'minutes',\n",
       "  'tonight',\n",
       "  'sincerely',\n",
       "  'an',\n",
       "  'asshole',\n",
       "  'that',\n",
       "  's',\n",
       "  'better',\n",
       "  'than',\n",
       "  'you',\n",
       "  'in',\n",
       "  'every',\n",
       "  'way']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    ## remove \\n \\t and non-alphanumeric\n",
    "    text = re.sub(\"(\\\\t|\\\\n)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    ## remove empty tokens\n",
    "    return [x.strip() for x in text.split(\" \") if len(x.strip()) > 0]\n",
    "\n",
    "nlp = gensim_api.load(\"word2vec-google-news-300\")\n",
    "unigram_corpus = []\n",
    "for s in clean[\"comment_text\"]:\n",
    "    unigram_corpus.append(clean_text(s))\n",
    "\n",
    "unigram_corpus[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: [1]\n",
      "severe_toxic: [0]\n",
      "obscene: [0]\n",
      "threat: [0]\n",
      "insult: [0]\n",
      "identity_hate: [0]\n"
     ]
    }
   ],
   "source": [
    "unseen = pd.DataFrame.from_dict(\n",
    "    {\"comment_text\": [\"go and fuck yourself and your mom too\"]}\n",
    ")\n",
    "for tox in toxic_labels:\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"vect\", CountVectorizer()),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            (\"classifier\", log_reg_models[tox]),\n",
    "        ]\n",
    "    )\n",
    "    pipe.fit(clean[\"comment_text\"], clean[tox])\n",
    "    print(f\"{tox}: {pipe.predict(unseen)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
